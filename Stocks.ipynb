{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f27d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install GetOldTweets3\n",
    "!pip install treeinterpreter\n",
    "import datetime\n",
    "import GetOldTweets3 as got\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "sentiment_i_a = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "#Get stock data for the given Stock Name\n",
    "def getStockDetails(stockname,start_time,end_time):\n",
    "  company = yf.Ticker(stockname)\n",
    "  company.info.get(\"longName\")\n",
    "  stockData = yf.download(stockname, start=start_time, end=end_time)\n",
    "  print(\"\\n Stock Data Obtained \")\n",
    "  print(stockData.head())\n",
    "  print(\"\\n\")\n",
    "  plt.title('Time series chart of Closing stocks for ' + company.info.get(\"longName\"))\n",
    "  plt.plot(stockData[\"Close\"])\n",
    "  plt.show()\n",
    "  print(\"\\n\")\n",
    "  stockData.to_csv('stockData_' + stockname + '.csv')\n",
    "#Method for data cleaning\n",
    "class TweetCleaner:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.punc_table = str.maketrans(\"\", \"\", string.punctuation) # to remove punctuation from each word in tokenize\n",
    "\n",
    "    def compound_word_split(self, compound_word):\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)\n",
    "        return [m.group(0) for m in matches]\n",
    "\n",
    "    def remove_non_ascii_chars(self, text):\n",
    "        return ''.join([w if ord(w) < 128 else ' ' for w in text])\n",
    "\n",
    "    def remove_hyperlinks(self,text):\n",
    "        return ' '.join([w for w in text.split(' ')  if not 'http' in w])\n",
    "\n",
    "    def get_cleaned_text(self, text):\n",
    "        cleaned_tweet = text.replace('\\\"','').replace('\\'','').replace('-',' ')\n",
    "        cleaned_tweet =  self.remove_non_ascii_chars(cleaned_tweet)\n",
    "        if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_tweet):\n",
    "            cleaned_tweet = cleaned_tweet[cleaned_tweet.index(':')+2:]\n",
    "        cleaned_tweet = self.remove_hyperlinks(cleaned_tweet)\n",
    "        cleaned_tweet = cleaned_tweet.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL') # to avoid being removed while removing punctuations\n",
    "        tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_tweet)] # remove punctuations and tokenize\n",
    "        tokens = [nltk.WordNetLemmatizer().lemmatize(w) for w in tokens if not w.lower() in self.stop_words and len(w)>1] # remove stopwords and single length words\n",
    "        cleaned_tweet = ' '.join(tokens)\n",
    "        cleaned_tweet = cleaned_tweet.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')\n",
    "        cleaned_tweet = cleaned_tweet\n",
    "        return cleaned_tweet\n",
    "\n",
    "    def clean_tweets(self, tweets, is_bytes = False):   \n",
    "        test_tweet_list = []\n",
    "        for tweet in tweets:\n",
    "            if is_bytes:\n",
    "                test_tweet_list.append(self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\")))\n",
    "            else:\n",
    "                test_tweet_list.append(self.get_cleaned_text(tweet))\n",
    "        return test_tweet_list\n",
    "    \n",
    "    def clean_single_tweet(self, tweet, is_bytes = False):  \n",
    "        if is_bytes:\n",
    "             return self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\"))\n",
    "        return self.get_cleaned_text(tweet)\n",
    "    \n",
    "    def cleaned_file_creator(self, op_file_name, value1, value2):\n",
    "        csvFile = open(op_file_name, 'w+')\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        for tweet in range(len(value1)):\n",
    "            csvWriter.writerow([value1[tweet], value2[tweet]])\n",
    "        csvFile.close()\n",
    "#Method for fetching tweets\n",
    "def fetchTweets(stockname,start_time,end_time):\n",
    "  csvFile = open('tweets_' + stockname + '.csv', 'a',encoding=\"utf-8\")\n",
    "  csvWriter = csv.writer(csvFile, lineterminator= '\\n')\n",
    "  cleanObj = TweetCleaner()\n",
    "  \n",
    "  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(stockname).setSince(start_time).setUntil(end_time).setTopTweets(\"true\") \n",
    "  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "  try:\n",
    "    for tweet in tweets:\n",
    "      tweet_text = tweet.text.encode('utf-8')\n",
    "      tweet_text = cleanObj.get_cleaned_text(tweet_text.decode())\n",
    "      tweetDate = tweet.date.date()\n",
    "      csvWriter.writerow([tweetDate, tweet.text])\n",
    "  except BaseException as e:\n",
    "      print('failed on_status,',str(e))\n",
    "#Method for processing tweets\n",
    "def processTweets(stockname):\n",
    "  columns=['Date','Tweets']\n",
    "  data = pd.DataFrame(columns)\n",
    "  df = pd.read_csv('tweets_' + stockname  + '.csv',encoding='utf-8', names=columns, header=None)\n",
    "  \n",
    "  indx=0\n",
    "  get_tweet=\"\"\n",
    "  #get tweets day wise\n",
    "  for i in range(0,len(df)-1):\n",
    "    get_date=df.Date.iloc[i]\n",
    "    next_date=df.Date.iloc[i+1]\n",
    "    if(str(get_date)==str(next_date)):\n",
    "      get_tweet = get_tweet + df.Tweets.iloc[i]+\" \"\n",
    "    if(str(get_date)!=str(next_date)):\n",
    "      data.at[indx,'Date'] = get_date\n",
    "      data.at[indx,'Tweets'] = get_tweet\n",
    "      indx=indx+1\n",
    "      get_tweet=\" \"\n",
    "\n",
    "  #get respective prices for each day using stockData\n",
    "  data['Prices']=\"\"\n",
    "  readStockData = pd.read_csv('stockData_' + stockname + '.csv')\n",
    "  readStockData.columns = [c.replace(' ', '_') for c in readStockData.columns]\n",
    "  for i in range (0,len(data)):\n",
    "      for j in range (0,len(readStockData)):\n",
    "          get_tweet_date = data.Date.iloc[i]\n",
    "          get_stock_date = readStockData.Date.iloc[j]\n",
    "          if(str(get_stock_date)==str(get_tweet_date)):\n",
    "            data.at[i,'Prices'] = int(readStockData.Adj_Close[j])\n",
    "            break\n",
    "\n",
    "  #drop rows that do not have Price values\n",
    "  data['Prices'].replace('', np.nan, inplace=True)\n",
    "  data.dropna(subset=['Prices'], inplace=True)\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  data['Prices'] = data['Prices'].apply(np.int64)\n",
    "  data.drop(0, 1, inplace=True)\n",
    "  print(data.head())\n",
    "  data.to_csv('processedTweets_' + stockname  + '.csv')\n",
    "#Method for sentiment analysis for tweets\n",
    "def sentimentAnalysis(stockname):\n",
    "  data = pd.read_csv('processedTweets_' + stockname  + '.csv', encoding='utf-8')\n",
    "  data[\"Comp\"] = ''\n",
    "  data[\"Negative\"] = ''\n",
    "  data[\"Neutral\"] = ''\n",
    "  data[\"Positive\"] = ''\n",
    "  for indexx, row in data.T.iteritems():\n",
    "    try:\n",
    "      sentence_i = unicodedata.normalize('NFKD', data.loc[indexx, 'Tweets'])\n",
    "      sentence_sentiment = sentiment_i_a.polarity_scores(sentence_i)\n",
    "      data.at[indexx, 'Comp'] =  sentence_sentiment['compound']\n",
    "      data.at[indexx, 'Negative'] = sentence_sentiment['neg']\n",
    "      data.at[indexx, 'Neutral'] =  sentence_sentiment['neu']\n",
    "      data.at[indexx, 'Positive'] = sentence_sentiment['pos'] \n",
    "    except TypeError:\n",
    "      print('failed on_status,',str(e))\n",
    "\n",
    "  data.drop(['Unnamed: 0'], 1, inplace=True)\n",
    "  print(data.head())\n",
    "  data.to_csv('sentimentAnalysis_' + stockname  + '.csv')\n",
    "\n",
    "  posi=0\n",
    "  nega=0\n",
    "  neutral = 0\n",
    "  for i in range (0,len(data)):\n",
    "    get_val = data.Comp[i]\n",
    "    if(float(get_val)<(0)):\n",
    "        nega=nega+1\n",
    "    if(float(get_val>(0))):\n",
    "        posi=posi+1\n",
    "    if(float(get_val)==(0)):\n",
    "        neutral=neutral+1\n",
    "  \n",
    "  posper=(posi/(len(data)))*100\n",
    "  negper=(nega/(len(data)))*100\n",
    "  neutralper=(neutral/(len(data)))*100\n",
    "\n",
    "  arr=np.asarray([posper,negper,neutralper], dtype=int)\n",
    "  plt.figure()\n",
    "  plt.pie(arr,labels=['positive','negative', 'neutral'])\n",
    "  plt.plot()\n",
    "\n",
    "  print(\"% of positive tweets= \",posper)\n",
    "  print(\"% of negative tweets= \",negper)\n",
    "  print(\"% of neutral tweets= \",neutralper)\n",
    "  \n",
    "#Method for stock price prediction using random forest model\n",
    "def RandomForestModel(stockname):\n",
    "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
    "  train, test = train_test_split(df, shuffle=False, test_size=0.2)\n",
    "\n",
    "  sentiment_score_list_train = []\n",
    "  for date, row in train.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_train.append(sentiment_score)\n",
    "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
    "\n",
    "  sentiment_score_list_test = []\n",
    "  for date, row in test.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_test.append(sentiment_score)\n",
    "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
    "\n",
    "  y_train = pd.DataFrame(train['Prices'])\n",
    "  y_test = pd.DataFrame(test['Prices'])\n",
    "\n",
    "  rf = RandomForestRegressor()\n",
    "  rf.fit(numpy_df_train, y_train)\n",
    "  prediction, bias, contributions = ti.predict(rf, numpy_df_test)\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  plt.figure()\n",
    "  plt.plot(test['Prices'].iloc[:].values)\n",
    "  plt.plot(prediction.flatten())\n",
    "  plt.title('Random Forest predicted prices')\n",
    "  plt.ylabel('Stock Prices')\n",
    "  plt.xlabel('Days')\n",
    "  plt.legend(['actual', 'predicted'])\n",
    "  plt.show()\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(\"RMSE value for Random Forest Model : \")\n",
    "  rmse = sqrt(mean_squared_error(y_test, prediction.flatten()))\n",
    "  print(rmse)\n",
    "  print(\"\\n\\n\")\n",
    "#Method for stock price prediction using support vector regression model\n",
    "def SVRModel(stockname):\n",
    "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
    "  train, test = train_test_split(df, shuffle=False, test_size=0.2)\n",
    "\n",
    "  sentiment_score_list_train = []\n",
    "  for date, row in train.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_train.append(sentiment_score)\n",
    "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
    "\n",
    "  sentiment_score_list_test = []\n",
    "  for date, row in test.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_test.append(sentiment_score)\n",
    "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
    "\n",
    "  y_train = pd.DataFrame(train['Prices'])\n",
    "  y_test = pd.DataFrame(test['Prices'])\n",
    "\n",
    "  svr_rbf = SVR(kernel='rbf', C=1e6, gamma=0.1)\n",
    "  svr_rbf.fit(numpy_df_train, y_train.values.flatten())\n",
    "  output_test_svm = svr_rbf.predict(numpy_df_test)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(test['Prices'].iloc[:].values)\n",
    "  plt.plot(output_test_svm)\n",
    "  plt.title('SVM predicted prices')\n",
    "  plt.ylabel('Stock Prices')\n",
    "  plt.xlabel('Days')\n",
    "  plt.legend(['actual', 'predicted'])\n",
    "  plt.show()\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(\"RMSE value for Support Vector Regression Model : \")\n",
    "  rmse = sqrt(mean_squared_error(y_test, output_test_svm))\n",
    "  print(rmse)\n",
    "  print(\"\\n\\n\")\n",
    "def main():\n",
    "  name = input(\"Enter a valid STOCKNAME of the Corporation: \") #enter the name of the company\n",
    "  start_date = input(\"Enter the Start Date in the following format[YYYY-MM-DD]: \") #enter the start date to fetch the tweets\n",
    "  end_date = input(\"Enter the End Date in the following format[YYYY-MM-DD]: \" ) #enter the end date to fetch the tweets\n",
    "  \n",
    "  if(len(name) > 0):\n",
    "    STOCKNAME  = name\n",
    "  else:\n",
    "    STOCKNAME = \"AAPL\"\n",
    "  \n",
    "  if(len(start_date) > 0):\n",
    "    start_time = start_date\n",
    "  else:\n",
    "    start_time = \"2018-01-01\"\n",
    "  \n",
    "  if(len(end_date) > 0):\n",
    "    end_time = end_date\n",
    "  else:\n",
    "    end_time = \"2019-12-31\"\n",
    "\n",
    "\n",
    "  #Get Stock Details\n",
    "  print(\"------------------------------ Getting Stock details -----------------------------\")\n",
    "  stockData = getStockDetails(STOCKNAME,start_time,end_time)\n",
    "  print(\"Stock Details fetched! \\n\")\n",
    "\n",
    "  #Fetching tweets\n",
    "  print(\"------------------------------ Fetching Tweets -----------------------------\")\n",
    "  fetchTweets(STOCKNAME,start_time,end_time)\n",
    "  print(\"Tweets fetched! \\n\")\n",
    "\n",
    "  #Get tweets Per Day and get the stock closing values for each date\n",
    "  print(\"------------------------------ Processing Tweets -----------------------------\")\n",
    "  processTweets(STOCKNAME)\n",
    "  print(\"Processed Tweets ! \\n\")\n",
    "\n",
    "  #Perform Sentiment Analysis\n",
    "  print(\"------------------------------ Performing Sentiment Analysis -----------------------------\")\n",
    "  sentimentAnalysis(STOCKNAME)\n",
    "  print(\"Completed Sentiment Analysis on Tweets ! \\n\\n\")\n",
    "  #time.sleep(10);\n",
    "\n",
    "  #Training and Predicting using Random Forest Regression Model\n",
    "  print(\"--------  Training and Predicting using Random Forest Regression Model -------\")\n",
    "  RandomForestModel(STOCKNAME)\n",
    "  print(\"\\n \\n\")\n",
    "\n",
    "  #Training and Predicting using Support Vecor Regression Model\n",
    "  print(\"-------- Training and Predicting using Support Vector Regression Model ------------\")\n",
    "  SVRModel(STOCKNAME)\n",
    "  print(\"\\n \\n\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9242b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d692d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
